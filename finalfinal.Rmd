---
title: "STAT 432 Final Project"
author: "Ayana Patidar (ayanap2), Ram Goenka (rgoenka2)"
date: "2024-12-13"
output: pdf_document
header-includes:
  - \usepackage{xcolor}
  - \usepackage{sectsty}
  - \sectionfont{\color{purple}}  # Change section title color
  - \subsectionfont{\color{red}}  # Change subsection title color (optional)
---

# Project Description and Abstract

Note: We have set up a GitHub repository with all of our code and data cleaning files and results as well as other codes that might not be included in this report due to length constrains. This repository may be access by clicking [here]. 

This project report was compiled for the submission of the final project for STAT 432 in the Fall 2024 semester at UIUC. Our main and broad goal lies within applying various methods covered during the semester to produce reasonable insights about the [Used Car Price Prediction Dataset](<https://www.kaggle.com/datasets/taeefnajib/used-car-price-prediction-dataset>) to predict the prices of used cars using various features from within the data which include numerical variables such as Model Year, Mileage, Horsepower, Cylinders, Engine size, as well as several categorical variables present. Our goal is to apply what we have learned and while our results might not be the most accurate, we aim to provide a holistic approach to the problem at hand. 

We have broken our approach into four primary steps:
\begin{itemize}
\item Data Pre-processing and summary statistics: [Explain here]
\item Unsupervised learning: For this section we considered and implemented three models which include \texttt{K-Means Clustering, Hierarchical Clustering, Spectral Clustering}. Using these methods we provide further insights into the data we are working with, building upon the previous section
\item Prediction Models: For this section we have chosen to implement models of various classifications, these include \texttt{Lasso Regression, k-Nearest Neighbors, Random Forest, Single Regression Tree} and \texttt{Support Vector Regression}. We utilize the findings from each of these models individually to comment on our findings.
\item Open Ended Question: We final consider a scenario where a researcher is interested in estimating the original (release) price of the cars in the dataset as if they were new. This question requires extrapolation so we consider some baseline methods such as model types, inflation rates something something...
\end{itemize}

Results here...

All the text in the report is by the group members solely. AI tools such as ChatGPT and Claude.AI were utilized for the purpose of understanding error messages in R and debugging steps as our code increased in complexity. 

\newpage

# Literature Review

The first paper that we will be reviewing is \textit{"How much is my car worth? A methodology for predicting used cars prices using Random Forest"}. In this paper, the author's propose a machine learning-based approach to predict the used car prices. The author's used the Kaggle dataset for used car price prediction. 

\newpage

# Data Processing and Summary Statistics 

In this section we will be discussing the data processing steps taken prior to performing any of the unsupervised or prediction tasks. We will discussing methodologies, share outputs of the code we utilized as well as discuss some summary statistics from the data. 

STUFF GOES HERE

```{r, echo=FALSE, results='hide', message=FALSE, warning=FALSE, cache = TRUE}
library(dplyr)
library(tidyr)
library(ggplot2)

data <- read.csv("used_cars.csv")

data$Horsepower = as.numeric(gsub("HP.*", "", data$engine))
data$Engine_Size = as.numeric(sub(".*([0-9]\\.[0-9]+)L.*", "\\1", data$engine))
data$Cylinders = as.numeric(sub(".*([0-9]+) Cylinder.*", "\\1", data$engine))
data$Mileage = as.numeric(gsub("[,mi.]", "", data$milage))
data$Price = as.numeric(gsub("[$,]", "", data$price))

data$Engine_Size[is.na(data$Engine_Size)] = mean(data$Engine_Size, 
                                                 na.rm = TRUE)
data$Horsepower[is.na(data$Horsepower)] = mean(data$Horsepower, 
                                               na.rm = TRUE)
mode_cylinders = as.numeric(names(sort(table(data$Cylinders), 
                                       decreasing = TRUE))[1])
data$Cylinders[is.na(data$Cylinders)] <- mode_cylinders
data$clean_title[data$clean_title == ""] = NA
data$clean_title[is.na(data$clean_title)] = "No"
data$fuel_type[data$fuel_type == "–"] = NA
data$fuel_type[data$fuel_type == "not supported"] = NA
data = data[!is.na(data$fuel_type), ]
data$transmission[data$transmission == "–"] = NA
data = data[!is.na(data$transmission), ]
data$int_col[data$int_col == "–"] = NA
data$int_col[is.na(data$int_col)] = "Unknown"
data$price[data$price == ""] = NA
data = data[, !(names(data) %in% c("engine", "milage", "price"))]

summary_table = data %>%
  summarise(
    Mean_Price = mean(Price, na.rm = TRUE),
    StdDev_Price = sd(Price, na.rm = TRUE),
    Min_Price = min(Price, na.rm = TRUE),
    Max_Price = max(Price, na.rm = TRUE),
    Missing_Price = mean(is.na(Price)) * 100,

    Mean_Mileage = mean(Mileage, na.rm = TRUE),
    StdDev_Mileage = sd(Mileage, na.rm = TRUE),
    Min_Mileage = min(Mileage, na.rm = TRUE),
    Max_Mileage = max(Mileage, na.rm = TRUE),
    Missing_Mileage = mean(is.na(Mileage)) * 100
  )

print(summary_table)

fuel_frequency = data %>%
  group_by(fuel_type) %>%
  summarise(Count = n()) %>%
  mutate(Percentage = (Count / sum(Count)) * 100)
fuel_frequency

# ggplot(data, aes(x = Price)) +
#   geom_histogram(binwidth = 5000, fill = "blue", color = "black") +
#   labs(title = "Price Distribution", x = "Price (USD)", y = "Frequency") +
#   theme_minimal()
# 
# ggplot(data, aes(x = Mileage, y = Price)) +
#   geom_point(color = "red", alpha = 0.6) +
#   labs(title = "Price vs. Mileage", x = "Mileage (mi)", y = "Price (USD)") +
#   theme_minimal()

data = read.csv("processed_used_cars.csv")
model <- lm(Price ~ ., data = data)
cooks_dist <- cooks.distance(model)
# 
# plot(cooks_dist, main = "Cook's Distance", type = "h", ylab = "Cook's Distance")
# abline(h = 4 / nrow(data), col = "red", lty = 2) 
threshold <- 4 / nrow(data)

influential_points <- which(cooks_dist > threshold)

data_cleaned <- data[-influential_points, ]
write.csv(data_cleaned, "final_data.csv", row.names = FALSE)
par(mfrow = c(1, 2))
# boxplot(data$Price, main = "Original Price Data", horizontal = TRUE)
# boxplot(data_cleaned$Price, main = "Cleaned Price Data", horizontal = TRUE)
```

# Unsupervised Learning

In this section we will be discussing our methods of implementation for each of the aforementioned unsupervised learning methods, provide relevant outputs as well we provide interpretations for the results we obtained.

\textbf{K-Means Clustering}: We begin with K-Means. We utilize the in-built ``kmeans()`` function from the ``cluster`` package. We will use $k$ values from 2 to 15 to have multiple values to choose from. To begin, we will scale the values using the ``scale()`` function to ensure that each point contributes equally to the distance calculations. We will be printing out the elbow plot to utilize the elbow method to estimate the optimal number clusters $k$. We will also be using an additional method called the "silhouette width" which is utilized to assign a silhouette width to each of the values of $k$. The width ranges from -1 to 1. The closer the width to 1, the more optimal the cluster value $k$. A high score shows that the data point us matched well to its given cluster. Motivation for this method was found from [\textcolor{blue}{this}](https://builtin.com/data-science/elbow-method) article.
```{r, cache = TRUE, message=FALSE, fig.width=7, fig.height=3, fig.align='center', echo=FALSE, warning = FALSE} 
library(cluster)

df <- read.csv("final_data.csv")
numeric_vars <- df %>% select_if(is.numeric)
scaled_data <- scale(numeric_vars)
wss <- numeric()
k_values <- 2:15
for (k in k_values) {
  set.seed(123)
  km <- kmeans(scaled_data, centers = k, nstart = 25)
  wss[k - 1] <- km$tot.withinss
}

plot(k_values, wss, type = "b", pch = 19, frame = FALSE,
     xlab = "k", ylab = "within-clusters sum of squares",
     main = "Elbow method for choosing best k")
```
Using the elbow method we can see that while there are some candidates ($k = 6, 7, 8$) for optimal cluster ($k$) values, there is no one specific value that sticks out as the best. Before looking at the results of the silhouette width, we choose $k = 7$ as the optimal cluster value. We now look at the results from the implementation of the silhouette width method:
```{r, cache = TRUE, message=FALSE, fig.width=7, fig.height=3, fig.align='center', echo=FALSE, warning = FALSE} 
library(cluster)

df <- read.csv("final_data.csv")
numeric_vars <- df %>% select_if(is.numeric)
scaled_data <- scale(numeric_vars)
wss <- numeric()
k_values <- 2:15
for (k in k_values) {
  set.seed(123)
  km <- kmeans(scaled_data, centers = k, nstart = 25)
  wss[k - 1] <- km$tot.withinss
}

avg_sil <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  set.seed(123)
  km_res <- kmeans(scaled_data, centers = k_values[i], nstart = 25)
  sil <- silhouette(km_res$cluster, dist(scaled_data))
  avg_sil[i] <- mean(sil[, 3])
}

m <- max(avg_sil)

sil_results <- data.frame(k = k_values, avg_sil_width = avg_sil)
plot(sil_results$k, sil_results$avg_sil_width, type = "b", pch = 19, frame = FALSE,
     xlab = "k", ylab = "Average Silhouette Width",
     main = "Silhouette width by k")
best_k <- sil_results$k[which.max(sil_results$avg_sil_width)]
best_k
```
As can be seen from the output of the code and the plot, our choice of $k = 7$ from the elbow method is validated using the silhouette width. We now visualize the clusters with this choice of $k$. 
```{r, cache = TRUE, message=FALSE, fig.width=7, fig.height=3, fig.align='center', echo=FALSE, warning = FALSE} 
library(tidyverse)
library(cluster)
library(factoextra)
library(ggplot2)

df <- read.csv("final_data.csv")
numeric_vars <- df %>% select_if(is.numeric)
scaled_data <- scale(numeric_vars)

set.seed(123)
km_res <- kmeans(scaled_data, centers = 7, nstart = 25)

pca_res <- prcomp(scaled_data)
pca_df <- as.data.frame(pca_res$x[, 1:2])
pca_df$cluster <- as.factor(km_res$cluster)

ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(alpha = 0.6) +
  theme_minimal() +
  labs(title = "K-means Clustering (k=7)",
       subtitle = "Visualized using first two principal components",
       x = "First Principal Component",
       y = "Second Principal Component",
       color = "Cluster") +
  scale_color_brewer(palette = "Set1")
```
From this visualization of the clusters, we can see that, for the most part, the clusters are well defined and distinct in nature with some overlap, which is to be expected as the silhouette width for $k = 7$ was $\sim 0.315$ which is closer to 0 than it is to 1. 

\textbf{Hierarchical Clustering}: Blah Blah

\textbf{Spectral Clustering}: Let us consider spectral clustering on the most relevant numerical variables available to us, to try and find some thresholds with which to cluster the data. 

# Prediction Models

In this section we will be discussing our methods of implementation for each of the aforementioned prediction models, provide relevant outputs as well we provide interpretations for the results we obtained.

\textbf{Lasso Regression}: For the implementation we use the ``glmnet`` package. For the tuning process, we focused on finding the optimal $\lambda$ value through $k$-fold cross-validation using ``cv.glmnet``. For the preprocessing steps, we split the data into numeric variables and categorical variables. We converted the categorical variables into dummy variables. We allocated 80% of the dataset for training the model and 20% as the test set. Upon choosing the best $\lambda$ value, we fit a final model and report the metrics. 
```{r, message=FALSE, fig.width=7, fig.height=3, fig.align='center', echo=FALSE, warning = FALSE, results = 'hide'} 

library(glmnet)
library(tidyverse)
library(caret)

df <- read.csv("final_data.csv")
numeric_predictors <- c("model_year", "Horsepower", 
                        "Engine_Size", "Cylinders", "Mileage")
X <- df[, numeric_predictors]
y <- df$Price

categorical_vars <- c("brand", "fuel_type", "transmission", "ext_col", "int_col", "accident", "clean_title")
X_cat <- model.matrix(~ ., data = df[, categorical_vars])[, -1] 
X_full <- cbind(as.matrix(X), X_cat)

set.seed(123)
train_index <- createDataPartition(y, p = 0.8, list = FALSE)
X_train <- X_full[train_index, ]
X_test <- X_full[-train_index, ]
y_train <- y[train_index]
y_test <- y[-train_index]

cv_lasso <- cv.glmnet(X_train, y_train, alpha = 1)
best_lambda <- cv_lasso$lambda.min
print(paste("Best lambda:", best_lambda))

final_model <- glmnet(X_train, y_train, alpha = 1, 
                      lambda = best_lambda)

y_pred_train <- predict(final_model, newx = X_train)

mse_train <- mean((y_train - y_pred_train)^2)
rmse_train <- sqrt(mse_train)
r2_train <- 1 - sum((y_train - y_pred_train)^2) / sum((y_train - mean(y_train))^2)

y_pred_test <- predict(final_model, newx = X_test)

mse_test <- mean((y_test - y_pred_test)^2)
rmse_test <- sqrt(mse_test)
r2_test <- 1 - sum((y_test - y_pred_test)^2) / 
  sum((y_test - mean(y_test))^2)

coef_matrix <- coef(final_model)
non_zero_coefs <- coef_matrix[which(coef_matrix != 0), ]
```

```{r, cache = TRUE, message=FALSE, fig.width=7, fig.height=3, fig.align='center', echo=FALSE, warning = FALSE} 
best_lambda <- cv_lasso$lambda.min
print(paste("Best lambda:", best_lambda))

plot_data_test <- data.frame(
  Actual = y_test,
  Predicted = as.vector(y_pred_test),
  Set = "Test"
)

plot_data_train <- data.frame(
  Actual = y_train,
  Predicted = as.vector(y_pred_train),
  Set = "Train"
)

plot_data <- rbind(plot_data_train, plot_data_test)
ggplot(plot_data, aes(x = Actual, y = Predicted, 
                      color = Set)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = 0, slope = 1, color = "red",
              linetype = "dashed") +
  theme_minimal() +
  labs(title = "Actual vs Predicted Car Prices",
       x = "Actual Price",
       y = "Predicted Price") +
  scale_color_manual(values = c("Train" = "blue", "Test" = "green")) +
  coord_equal()

plot(cv_lasso)
```
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Metric}          & \textbf{Training} & \textbf{Test} \\ \hline
RMSE                     & 21854.21          & 59611.71      \\ \hline
R-squared                & 0.872             & 0.3185        \\ \hline
\end{tabular}
\caption{Training and Test Metrics for Lasso}
\label{tab:metrics}
\end{table}


